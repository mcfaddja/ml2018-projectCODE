import readline
import scipy.sparse
import scipy
import numpy as np
import pandas as pd
from sklearn.feature_extraction import DictVectorizer
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn import svm
from sklearn.svm import SVC, LinearSVC
from sklearn.svm import SVR, LinearSVR
from sklearn import linear_model
from sklearn.externals import joblib
import timeit
import time
likes = pd.read_csv("/home/jamster/data/training/relation/relation.csv")
likesUIDs = likes.ix[:,1].values
likesLIDs = likes.ix[:,2].values
lsLikesUIDs = likesUIDs.tolist()
lsLikesLIDs = likesLIDs.tolist()
setLikesUIDs = set(lsLikesUIDs)
setLikesLIDs = set(lsLikesLIDs)
unqLikesUIDs = (list(setLikesUIDs))
unqLikesLIDs = (list(setLikesLIDs))
allLikesLS = [lsLikesUIDs, [str(x) for x in lsLikesLIDs]]
allLikesLS = list(map(list, zip(*allLikesLS)))
likes = None
aDictLikes2 = {}
for aUID in unqLikesUIDs:
	aDictLikes2[aUID]=[]
for row in allLikesLS:
	aDictLikes2[row[0]].append(row[1])
combDICT = {}
for uid in unqLikesUIDs:
	tmpDICT={}
	tmpLS = aDictLikes2[uid]
	for row in tmpLS:
		tmpDICT[str(row)]=1
	combDICT[uid]=tmpDICT
tryTHIS=[]
for uid in unqLikesUIDs:
	tryTHIS.append(combDICT[uid])
v = DictVectorizer()
likesMAT=v.fit_transform(tryTHIS)
likesUIDs=None
likesLIDs=None
lsLikesUIDs=None
lsLikesLIDs=None
setLikesUIDs=Nonoe
setLikesUIDs=None
setLikesLIDs=None
unqLikesUIDs=None
unqLikesLIDs=None
allLikesLS=None
aDictLikes2=None
combDICT=None
tryTHIS=None
v=None
profilesDF=pd.read_csv("/home/jamster/data/training/profile/profile.csv")
profilesDF=None
from scipy.sparse.linalg import svds, eigs
combLIKES=likesMAT.transpose()*likesMAT
combLikes=combLIKES.tocsc()
likesMAT=None
from scipy.sparse.linalg import svds, eigs
u, s, vt = svds(combLIKES, k=1000)
print(u.shape)
np.savetxt('/home/jamster/u1000.csv', u, delimiter=",")
np.savetxt('/home/jamster/s1000.csv', s, delimiter=",")
np.savetxt('/home/jamster/vt1000.csv', vt, delimiter=",")
u = None
vt = None
s = None
combLIKES=None
quit()
from scipy.sparse import csc_matrix
from scipy.sparse.linalg import svds, eigs
A = csc_matrix([[1, 0, 0], [5, 0, 2], [0, -1, 0], [0, 0, 3]], dtype=float)
s = svds(A, k=2, return_singular_vectors=False)
print(s)
quit()
import readline
import scipy.sparse
import scipy
import numpy as np
import pandas as pd
from sklearn.feature_extraction import DictVectorizer
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn import svm
from sklearn.svm import SVC, LinearSVC
from sklearn.svm import SVR, LinearSVR
from sklearn import linear_model
from sklearn.externals import joblib
import timeit
import time
likes = pd.read_csv("/home/jamster/data/training/relation/relation.csv")
likesUIDs = likes.ix[:,1].values
likesLIDs = likes.ix[:,2].values
lsLikesUIDs = likesUIDs.tolist()
lsLikesLIDs = likesLIDs.tolist()
setLikesUIDs = set(lsLikesUIDs)
setLikesLIDs = set(lsLikesLIDs)
unqLikesUIDs = (list(setLikesUIDs))
unqLikesLIDs = (list(setLikesLIDs))
allLikesLS = [lsLikesUIDs, [str(x) for x in lsLikesLIDs]]
allLikesLS = list(map(list, zip(*allLikesLS)))
aDictLikes2 = {}
for aUID in unqLikesUIDs:
	aDictLikes2[aUID]=[]
for row in allLikesLS:
	aDictLikes2[row[0]].append(row[1])
combDICT = {}
for uid in unqLikesUIDs:
	tmpDICT={}
	tmpLS = aDictLikes2[uid]
	for row in tmpLS:
		tmpDICT[str(row)]=1
	combDICT[uid]=tmpDICT
tryTHIS=[]
for uid in unqLikesUIDs:
	tryTHIS.append(combDICT[uid])
v = DictVectorizer()
likesMAT=v.fit_transform(tryTHIS)
from scipy.sparse.linalg import eigs, svds
s = svds(likesMAT, k=9000, /data/training/relation/relation.csv")
likesUIDs = likes.ix[:,1].values
likesLIDs = likes.ix[:,2].values
lsLikesUIDs = likesUIDs.tolist()
lsLikesLIDs = likesLIDs.tolist()
setLikesUIDs = set(lsLikesUIDs)
setLikesLIDs = set(lsLikesLIDs)
s = svds(likesMAT, k=9000, return_singular_vectors=False)
quit()
import readline
import scipy.sparse
import scipy
import numpy as np
import pandas as pd
from sklearn.feature_extraction import DictVectorizer
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn import svm
from sklearn.svm import SVC, LinearSVC
from sklearn.svm import SVR, LinearSVR
from sklearn import linear_model
from sklearn.externals import joblib
import timeit
import time
likes = pd.read_csv("/home/jamster/data/training/relation/relation.csv")
likesUIDs = likes.ix[:,1].values
likesLIDs = likes.ix[:,2].values
lsLikesUIDs = likesUIDs.tolist()
lsLikesLIDs = likesLIDs.tolist()
setLikesUIDs = set(lsLikesUIDs)
setLikesLIDs = set(lsLikesLIDs)
unqLikesUIDs = (list(setLikesUIDs))
unqLikesLIDs = (list(setLikesLIDs))
allLikesLS = [lsLikesUIDs, [str(x) for x in lsLikesLIDs]]
allLikesLS = list(map(list, zip(*allLikesLS)))
aDictLikes2 = {}
for aUID in unqLikesUIDs:
	aDictLikes2[aUID]=[]
for row in allLikesLS:
	aDictLikes2[row[0]].append(row[1])
combDICT = {}
for uid in unqLikesUIDs:
	tmpDICT={}
	tmpLS = aDictLikes2[uid]
	for row in tmpLS:
		tmpDICT[str(row)]=1
	combDICT[uid]=tmpDICT
tryTHIS=[]
for uid in unqLikesUIDs:
	tryTHIS.append(combDICT[uid])
v = DictVectorizer()
likesMAT=v.fit_transform(tryTHIS)
print(min(likesMAT.shape))
print((min(likesMAT.shape) ** 2) * max(likesMAT.shape))
print((max(likesMAT.shape) ** 2) * min(likesMAT.shape))
print((min(likesMAT.shape) * max(likesMAT.shape) * 6000)
)
print((min(likesMAT.shape) * max(likesMAT.shape) * 9000)
)
from scipy.sparse.linalg import svds, eigs
times = []
for i in range(4):
	t0 = time.time()
	tmpK = 2 ** (i+1)
	s = svds(likesMAT, k=tmpK, return_singular_vectors=False)
	tTOT = time.time() - t0
	print(tTOT)
for i in range(6):
	tmpK = 2 ** (i+1)
	print(tmpK)
for i in range(6):
	tmpK = 3 ** (i+1)
	print(tmpK)
for i in range(5):
	tmpK = 4 ** (i+1)
	print(tmpK)
for i in range(5):
	tmpK = 5 ** (i+1)
	print(tmpK)
nOFk=[2,3,4,5,6,8,9,10,12,15,16,18,20,24,25,27,32]
for row in nOFk:
times=[]
for row in nOFk:
	t0=time.time()
	s = svds(likesMAT, k=row, return_singular_vectors=False)
	tTOT = time.time() - t0
	times.append(tTOT)
	print(tTOT)
print(nOFk
)
for row in nOFk:
	print(row)
for row in nOFk:
	t0=time.time()
	s = svds(likesMAT, k=row, return_singular_vectors=False)
	tTOT = time.time() - t0
	times.append(tTOT)
	print(tTOT)
for i in range(3):
	for row in nOFk:
		t0=time.time()
		s = svds(likesMAT, k=row, return_singular_vectors=False)
		tTOT=time.time()-t0
		times.append(tTOT)
		print(tTOT)
	print(' ')
numCMDs=readline.get_current_history_length(_
numCMDs=readline.get_current_history_length()
